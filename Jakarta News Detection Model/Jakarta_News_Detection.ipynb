{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_jakarta.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIeoW7K96S6k"
      },
      "source": [
        "# ML Biasa (Jakarta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4K91DUi6TxC"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLDui8wP4_cG"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel('/content/data_berita_content.xlsx')\n",
        "dic = pd.read_csv('/content/normalize_word.csv')\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "stop_words = []\n",
        "stopword_data = urllib.request.urlopen('http://static.hikaruyuuki.com/wp-content/uploads/stopword_list_tala.txt').read().decode('utf-8').split('\\n')\n",
        "for line in stopword_data:\n",
        "    stop_words.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMeF8b6Q5kku"
      },
      "source": [
        "data = data.replace({'Label Text Classification': {2: 1}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LckfDsWP5l-C"
      },
      "source": [
        "def normalize(text):\n",
        "    text = text.split()\n",
        "    for val in dic.itertuples(index=False):\n",
        "        text = [w.replace(val.slang, val.formal) if w == val.slang else w for w in text]\n",
        "    return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2VG5ccn5mto"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# bersihkan teks\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\r', ' ', text)\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9-h5YVG5ndo",
        "outputId": "640ba6f2-709f-4757-8cb7-d3016d60123c"
      },
      "source": [
        "%%time\n",
        "data['content'] = data['content'].apply(clean_text)\n",
        "data['content'] = data['content'].apply(normalize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 7s, sys: 89 ms, total: 1min 7s\n",
            "Wall time: 1min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ5_GVs05rQm"
      },
      "source": [
        "def checkLen(data):\n",
        "  return len(data.split())\n",
        "data['Len'] = data['content'].apply(checkLen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXQvZ6N75v4W",
        "outputId": "9e06e3ed-0b8b-4d41-c871-5cc0452e00eb"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYszAK_E51f-"
      },
      "source": [
        "data = data[data['Len'] > 10].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG6tBwSa53E4",
        "outputId": "2112acd2-1c5d-462a-c1ec-ab21a2b61458"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHJNZg7I56dV",
        "outputId": "492eec1e-82ae-46af-ecf7-101b04b4d448"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "df_train, df_test = train_test_split(data, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN size: 495\n",
            "TEST size: 124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pufD73wQ6QrD"
      },
      "source": [
        "## Buat bag of word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWPPKnOj6BAp"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# data = dataCheck.copy()\n",
        "# # Ubah fitur menjadi bentuk TFIDF dengan jumlah kata 150\n",
        "mx_feat = 500\n",
        "vectorizer = CountVectorizer(max_features=mx_feat)\n",
        "bow_data_train = vectorizer.fit_transform(df_train['content'])\n",
        "bow_data_train = pd.DataFrame.sparse.from_spmatrix(bow_data_train,columns=vectorizer.get_feature_names())\n",
        "\n",
        "bow_data_test = vectorizer.transform(df_test['content'])\n",
        "bow_data_test = pd.DataFrame.sparse.from_spmatrix(bow_data_test,columns=vectorizer.get_feature_names())\n",
        "# data = data.join(bow_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDe60fc16DHh",
        "outputId": "bb4a59b3-b6c8-451b-d6b8-31be5caa0b61"
      },
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(vectorizer, 'CountVectorizer.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CountVectorizer.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5bucZ4q6EM9",
        "outputId": "2cf80203-e305-44d0-e1a6-0cc8eb7047bf"
      },
      "source": [
        "bow_data_train.values.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(495, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swFwBMaW6FIV",
        "outputId": "c76e745a-c21a-48cc-a878-50381d109c13"
      },
      "source": [
        "bow_data_test.values.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_FFg3y6a2T"
      },
      "source": [
        "## NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXwwZsZD6W1y"
      },
      "source": [
        "x_train = bow_data_train.values\n",
        "x_test = bow_data_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnAedgQJ6bpC",
        "outputId": "e61ce432-107a-47f4-d2ef-1cdb243a6535"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train['Label Text Classification'].tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train['Label Text Classification'].tolist())\n",
        "y_test = encoder.transform(df_test['Label Text Classification'].tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train (495, 1)\n",
            "y_test (124, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0AD4jl16dmq"
      },
      "source": [
        "from keras.layers import Input, Dropout, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "dropout = 0.2\n",
        "def make_nn(dropout):\n",
        "  inputs = Input(shape=(mx_feat,))\n",
        "  layer = Dropout(dropout)(inputs)\n",
        "  layer = Dense(16, activation=\"relu\")(layer)\n",
        "  layer = Dense(16, activation=\"relu\")(layer)\n",
        "  layer = Dense(1, activation=\"sigmoid\")(layer)\n",
        "  model = Model(inputs, layer)\n",
        "  return model\n",
        "model = make_nn(dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK5VS4_C6ekK",
        "outputId": "fa6da988-d5e8-484b-d775-3990c41055dd"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 10), verbose=1)\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-6), metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train,validation_data=(x_test,y_test), batch_size=8, epochs=70, callbacks=[lr_schedule],verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
            "62/62 [==============================] - 1s 6ms/step - loss: 0.7392 - accuracy: 0.5377 - val_loss: 0.7072 - val_accuracy: 0.5000\n",
            "Epoch 2/70\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.2589254117941672e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7315 - accuracy: 0.5330 - val_loss: 0.7069 - val_accuracy: 0.5000\n",
            "Epoch 3/70\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5848931924611134e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6877 - accuracy: 0.5588 - val_loss: 0.7066 - val_accuracy: 0.5000\n",
            "Epoch 4/70\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 1.9952623149688796e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5793 - val_loss: 0.7062 - val_accuracy: 0.5000\n",
            "Epoch 5/70\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.51188643150958e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7004 - accuracy: 0.5260 - val_loss: 0.7056 - val_accuracy: 0.5000\n",
            "Epoch 6/70\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 3.162277660168379e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7125 - accuracy: 0.5788 - val_loss: 0.7051 - val_accuracy: 0.5161\n",
            "Epoch 7/70\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 3.981071705534972e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7097 - accuracy: 0.5095 - val_loss: 0.7043 - val_accuracy: 0.5081\n",
            "Epoch 8/70\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 5.011872336272722e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6826 - accuracy: 0.5623 - val_loss: 0.7034 - val_accuracy: 0.5161\n",
            "Epoch 9/70\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 6.309573444801933e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7055 - accuracy: 0.5452 - val_loss: 0.7022 - val_accuracy: 0.5081\n",
            "Epoch 10/70\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 7.943282347242815e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7108 - accuracy: 0.5288 - val_loss: 0.7006 - val_accuracy: 0.5081\n",
            "Epoch 11/70\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 9.999999999999999e-06.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5813 - val_loss: 0.6987 - val_accuracy: 0.5161\n",
            "Epoch 12/70\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 1.2589254117941675e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6649 - accuracy: 0.5848 - val_loss: 0.6965 - val_accuracy: 0.5161\n",
            "Epoch 13/70\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 1.584893192461113e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.5802 - val_loss: 0.6937 - val_accuracy: 0.5161\n",
            "Epoch 14/70\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 1.9952623149688796e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.5598 - val_loss: 0.6901 - val_accuracy: 0.5161\n",
            "Epoch 15/70\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 2.5118864315095795e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6764 - accuracy: 0.6239 - val_loss: 0.6862 - val_accuracy: 0.5161\n",
            "Epoch 16/70\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 3.162277660168379e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6987 - accuracy: 0.5545 - val_loss: 0.6815 - val_accuracy: 0.5242\n",
            "Epoch 17/70\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 3.9810717055349735e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5591 - val_loss: 0.6760 - val_accuracy: 0.5484\n",
            "Epoch 18/70\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 5.0118723362727224e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.6386 - val_loss: 0.6695 - val_accuracy: 0.5565\n",
            "Epoch 19/70\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 6.309573444801932e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.6570 - val_loss: 0.6617 - val_accuracy: 0.5806\n",
            "Epoch 20/70\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 7.943282347242814e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.6513 - val_loss: 0.6520 - val_accuracy: 0.6210\n",
            "Epoch 21/70\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 9.999999999999999e-05.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.6949 - val_loss: 0.6437 - val_accuracy: 0.6129\n",
            "Epoch 22/70\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012589254117941674.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5913 - accuracy: 0.7258 - val_loss: 0.6338 - val_accuracy: 0.6210\n",
            "Epoch 23/70\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00015848931924611142.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.6973 - val_loss: 0.6248 - val_accuracy: 0.5968\n",
            "Epoch 24/70\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00019952623149688788.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7204 - val_loss: 0.6118 - val_accuracy: 0.6048\n",
            "Epoch 25/70\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00025118864315095795.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5380 - accuracy: 0.7555 - val_loss: 0.6010 - val_accuracy: 0.6048\n",
            "Epoch 26/70\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00031622776601683794.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7873 - val_loss: 0.5893 - val_accuracy: 0.6532\n",
            "Epoch 27/70\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0003981071705534973.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4895 - accuracy: 0.7826 - val_loss: 0.5819 - val_accuracy: 0.6774\n",
            "Epoch 28/70\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005011872336272724.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8055 - val_loss: 0.5849 - val_accuracy: 0.6613\n",
            "Epoch 29/70\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.000630957344480193.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8276 - val_loss: 0.5838 - val_accuracy: 0.7016\n",
            "Epoch 30/70\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0007943282347242812.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3473 - accuracy: 0.8825 - val_loss: 0.5693 - val_accuracy: 0.7177\n",
            "Epoch 31/70\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2890 - accuracy: 0.8911 - val_loss: 0.5764 - val_accuracy: 0.7177\n",
            "Epoch 32/70\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0012589254117941675.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9113 - val_loss: 0.5821 - val_accuracy: 0.7177\n",
            "Epoch 33/70\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0015848931924611139.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.8893 - val_loss: 0.6602 - val_accuracy: 0.7097\n",
            "Epoch 34/70\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.001995262314968879.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1754 - accuracy: 0.9310 - val_loss: 0.7328 - val_accuracy: 0.6935\n",
            "Epoch 35/70\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00251188643150958.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1571 - accuracy: 0.9345 - val_loss: 0.8307 - val_accuracy: 0.6855\n",
            "Epoch 36/70\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0031622776601683794.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9435 - val_loss: 0.8246 - val_accuracy: 0.6855\n",
            "Epoch 37/70\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.003981071705534973.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9301 - val_loss: 0.9396 - val_accuracy: 0.6855\n",
            "Epoch 38/70\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.005011872336272725.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1931 - accuracy: 0.9400 - val_loss: 0.8718 - val_accuracy: 0.6935\n",
            "Epoch 39/70\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00630957344480193.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9125 - val_loss: 0.7145 - val_accuracy: 0.7258\n",
            "Epoch 40/70\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.007943282347242814.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.8971 - val_loss: 0.7671 - val_accuracy: 0.7339\n",
            "Epoch 41/70\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.01.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1897 - accuracy: 0.9307 - val_loss: 0.9004 - val_accuracy: 0.7258\n",
            "Epoch 42/70\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.012589254117941661.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9330 - val_loss: 0.7119 - val_accuracy: 0.6613\n",
            "Epoch 43/70\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.01584893192461114.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1548 - accuracy: 0.9483 - val_loss: 0.6952 - val_accuracy: 0.6613\n",
            "Epoch 44/70\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.01995262314968879.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2019 - accuracy: 0.9371 - val_loss: 1.2399 - val_accuracy: 0.6774\n",
            "Epoch 45/70\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.025118864315095822.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2637 - accuracy: 0.9105 - val_loss: 1.2303 - val_accuracy: 0.7258\n",
            "Epoch 46/70\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.03162277660168379.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3316 - accuracy: 0.8992 - val_loss: 0.7167 - val_accuracy: 0.6613\n",
            "Epoch 47/70\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.039810717055349686.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2000 - accuracy: 0.9207 - val_loss: 0.7378 - val_accuracy: 0.7177\n",
            "Epoch 48/70\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.05011872336272725.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9318 - val_loss: 1.4119 - val_accuracy: 0.6613\n",
            "Epoch 49/70\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0630957344480193.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.9153 - accuracy: 0.7607 - val_loss: 1.1490 - val_accuracy: 0.6532\n",
            "Epoch 50/70\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.07943282347242821.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4482 - accuracy: 0.5662 - val_loss: 0.8645 - val_accuracy: 0.5887\n",
            "Epoch 51/70\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.09999999999999999.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.5330 - val_loss: 0.7154 - val_accuracy: 0.5968\n",
            "Epoch 52/70\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.12589254117941662.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7908 - accuracy: 0.4712 - val_loss: 0.6707 - val_accuracy: 0.5968\n",
            "Epoch 53/70\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1584893192461114.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6821 - accuracy: 0.5320 - val_loss: 0.7157 - val_accuracy: 0.6210\n",
            "Epoch 54/70\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1995262314968879.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.3419 - accuracy: 0.5327 - val_loss: 0.7035 - val_accuracy: 0.4032\n",
            "Epoch 55/70\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.2511886431509582.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6980 - accuracy: 0.5411 - val_loss: 0.6852 - val_accuracy: 0.5968\n",
            "Epoch 56/70\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.3162277660168379.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6951 - accuracy: 0.4552 - val_loss: 0.6982 - val_accuracy: 0.5968\n",
            "Epoch 57/70\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.3981071705534969.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7111 - accuracy: 0.5461 - val_loss: 0.6816 - val_accuracy: 0.5968\n",
            "Epoch 58/70\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.5011872336272725.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7057 - accuracy: 0.5062 - val_loss: 0.6911 - val_accuracy: 0.5968\n",
            "Epoch 59/70\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.6309573444801929.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.5160 - val_loss: 0.8801 - val_accuracy: 0.4032\n",
            "Epoch 60/70\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.7943282347242822.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7556 - accuracy: 0.4613 - val_loss: 0.7256 - val_accuracy: 0.4032\n",
            "Epoch 61/70\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 1.0.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7294 - accuracy: 0.4841 - val_loss: 0.6859 - val_accuracy: 0.5968\n",
            "Epoch 62/70\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 1.258925411794166.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7715 - accuracy: 0.5645 - val_loss: 0.6996 - val_accuracy: 0.4032\n",
            "Epoch 63/70\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 1.584893192461114.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8052 - accuracy: 0.4871 - val_loss: 0.6765 - val_accuracy: 0.5968\n",
            "Epoch 64/70\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 1.9952623149688788.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7236 - accuracy: 0.5198 - val_loss: 0.7192 - val_accuracy: 0.5968\n",
            "Epoch 65/70\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 2.5118864315095824.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7461 - accuracy: 0.5621 - val_loss: 0.9870 - val_accuracy: 0.4032\n",
            "Epoch 66/70\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 3.1622776601683795.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8214 - accuracy: 0.4845 - val_loss: 0.7385 - val_accuracy: 0.5968\n",
            "Epoch 67/70\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 3.981071705534969.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.7705 - accuracy: 0.5244 - val_loss: 0.7390 - val_accuracy: 0.5968\n",
            "Epoch 68/70\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 5.011872336272725.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.5361 - val_loss: 0.8569 - val_accuracy: 0.5968\n",
            "Epoch 69/70\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 6.30957344480193.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.8929 - accuracy: 0.5085 - val_loss: 1.2069 - val_accuracy: 0.5968\n",
            "Epoch 70/70\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 7.943282347242822.\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 1.4036 - accuracy: 0.4590 - val_loss: 0.6805 - val_accuracy: 0.5968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "kL3x9DHo6fnJ",
        "outputId": "21daeb70-ae61-48cd-8537-ba6dc2b0f9ea"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel('learning rate')\n",
        "plt.ylabel('val_loss')\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"val_loss\"])\n",
        "plt.axis([3*1e-4, 3*1e-3, 0, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.00030000000000000003, 0.003, 0.0, 2.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjVZBqw16gcj",
        "outputId": "9c3aa538-fd7c-4fbc-af04-422a46bb75a5"
      },
      "source": [
        "model = make_nn(dropout)\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.002869), metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train,validation_data=(x_test,y_test), batch_size=8, epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "62/62 [==============================] - 1s 6ms/step - loss: 0.7322 - accuracy: 0.5249 - val_loss: 0.6129 - val_accuracy: 0.7097\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.7777 - val_loss: 0.5447 - val_accuracy: 0.7500\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8547 - val_loss: 0.6218 - val_accuracy: 0.7500\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.2383 - accuracy: 0.9075 - val_loss: 0.7047 - val_accuracy: 0.7339\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9441 - val_loss: 0.7343 - val_accuracy: 0.7500\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1801 - accuracy: 0.9291 - val_loss: 0.7751 - val_accuracy: 0.7581\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1834 - accuracy: 0.9362 - val_loss: 0.7592 - val_accuracy: 0.7339\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1050 - accuracy: 0.9642 - val_loss: 0.8571 - val_accuracy: 0.7500\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9559 - val_loss: 0.8993 - val_accuracy: 0.7500\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9674 - val_loss: 1.0537 - val_accuracy: 0.7500\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0649 - accuracy: 0.9725 - val_loss: 1.0899 - val_accuracy: 0.7258\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9637 - val_loss: 1.1185 - val_accuracy: 0.7339\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0793 - accuracy: 0.9676 - val_loss: 1.1446 - val_accuracy: 0.7419\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9651 - val_loss: 0.9508 - val_accuracy: 0.7742\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9852 - val_loss: 1.1714 - val_accuracy: 0.7500\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0443 - accuracy: 0.9826 - val_loss: 1.1683 - val_accuracy: 0.7661\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9834 - val_loss: 1.2200 - val_accuracy: 0.7500\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0496 - accuracy: 0.9795 - val_loss: 1.0955 - val_accuracy: 0.7500\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9769 - val_loss: 1.0883 - val_accuracy: 0.7742\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9849 - val_loss: 1.0762 - val_accuracy: 0.7823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNCAC_VN6ha8"
      },
      "source": [
        "X_pred = model.predict(x_test).round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZYiRYWg6iCj",
        "outputId": "a77e3fd7-149a-442f-f7b7-0ac40bc0da1a"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(X_pred,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.782258064516129"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCGuux1A6iwU",
        "outputId": "adce7c53-895d-4945-b53a-a369ae21400a"
      },
      "source": [
        "model.save('model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr82EkEk6jj7",
        "outputId": "10721048-5af1-4afc-ad1e-8d16780ee912"
      },
      "source": [
        "!zip -r /content/jakarta_class.zip /content/model/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/assets/ (stored 0%)\n",
            "  adding: content/model/saved_model.pb (deflated 88%)\n",
            "  adding: content/model/keras_metadata.pb (deflated 87%)\n",
            "  adding: content/model/variables/ (stored 0%)\n",
            "  adding: content/model/variables/variables.index (deflated 63%)\n",
            "  adding: content/model/variables/variables.data-00000-of-00001 (deflated 10%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Kz3qhZXA6kY0",
        "outputId": "b6c8ad38-5701-4687-f75e-2b1d57a5b44a"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/jakarta_class.zip\")\n",
        "files.download(\"/content/CountVectorizer.pkl\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7342cc5d-a714-4569-83c6-b2fbd171ab31\", \"jakarta_class.zip\", 110858)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0P3_f2fEoIp"
      },
      "source": [
        "# ML W2V (Jakarta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcyQqL4rEvqO"
      },
      "source": [
        "## Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w9hyOlkEzen"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_excel('/content/data_berita_content.xlsx')\n",
        "dic = pd.read_csv('/content/normalize_word.csv')\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "stop_words = []\n",
        "stopword_data = urllib.request.urlopen('http://static.hikaruyuuki.com/wp-content/uploads/stopword_list_tala.txt').read().decode('utf-8').split('\\n')\n",
        "for line in stopword_data:\n",
        "    stop_words.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmTBMqmJEz7Y"
      },
      "source": [
        "data = data.replace({'Label Text Classification': {2: 1}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4lDBQsGE-u1"
      },
      "source": [
        "def normalize(text):\n",
        "    text = text.split()\n",
        "    for val in dic.itertuples(index=False):\n",
        "        text = [w.replace(val.slang, val.formal) if w == val.slang else w for w in text]\n",
        "    return \" \".join(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWPNfzwpFFvp"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# bersihkan teks\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\r', ' ', text)\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Em5LIiCFJvD",
        "outputId": "c136fd01-d449-4877-b46a-8f766fbc3921"
      },
      "source": [
        "%%time\n",
        "data['content'] = data['content'].apply(clean_text)\n",
        "data['content'] = data['content'].apply(normalize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 8s, sys: 57 ms, total: 1min 8s\n",
            "Wall time: 1min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDN18ez-FNGe"
      },
      "source": [
        "def checkLen(data):\n",
        "  return len(data.split())\n",
        "data['Len'] = data['content'].apply(checkLen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BifZYtXIFOGQ",
        "outputId": "b4da374a-d483-484d-809f-64a6bc30ea43"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvU-iQ0bFPHX"
      },
      "source": [
        "data = data[data['Len'] > 10].reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwDBBGdvFQB0",
        "outputId": "86835f57-a7ec-4173-903b-38c721d7fa2e"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "619"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bs6OSL-1FUZE",
        "outputId": "85bfe461-d19b-49af-d048-64d0bfd83385"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TRAIN_SIZE = 0.8\n",
        "\n",
        "df_train, df_test = train_test_split(data, test_size=1-TRAIN_SIZE, random_state=42)\n",
        "print(\"TRAIN size:\", len(df_train))\n",
        "print(\"TEST size:\", len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN size: 495\n",
            "TEST size: 124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8FBuShsFdpv"
      },
      "source": [
        "## Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IWT6rTqFiQo",
        "outputId": "2f38eae2-f072-4183-c726-7ced78490929"
      },
      "source": [
        "%%time\n",
        "documents = [_content.split() for _content in df_train.content] "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.7 ms, sys: 0 ns, total: 10.7 ms\n",
            "Wall time: 12 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_bBwYHAFqCL"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# WORD2VEC \n",
        "W2V_SIZE = 300\n",
        "W2V_WINDOW = 7\n",
        "W2V_EPOCH = 32\n",
        "W2V_MIN_COUNT = 10\n",
        "\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n",
        "                                            window=W2V_WINDOW, \n",
        "                                            min_count=W2V_MIN_COUNT, \n",
        "                                            workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ0bY3M4FrV1"
      },
      "source": [
        "w2v_model.build_vocab(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP9_lHOBF6aQ",
        "outputId": "5546e2d2-38e8-44dd-ab94-18c2e0bfcb88"
      },
      "source": [
        "words = w2v_model.wv.vocab.keys()\n",
        "vocab_size = len(words)\n",
        "print(\"Vocab size\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size 2285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xYOAfDOF70X",
        "outputId": "77917d0d-d453-4bd6-d353-5a1f703b6a51"
      },
      "source": [
        "%%time\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.4 s, sys: 97.6 ms, total: 10.5 s\n",
            "Wall time: 5.7 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2434918, 3501856)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlq9sfecF9Hf",
        "outputId": "3acd09b2-3439-41ca-f28b-820e234d78bb"
      },
      "source": [
        "w2v_model.most_similar(\"jakarta\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('merdeka', 0.642341136932373),\n",
              " ('medan', 0.5640632510185242),\n",
              " ('pemprov', 0.5234548449516296),\n",
              " ('balai', 0.4976435899734497),\n",
              " ('selasa', 0.48176389932632446),\n",
              " ('rabu', 0.46575337648391724),\n",
              " ('–', 0.45659810304641724),\n",
              " ('timur', 0.44270867109298706),\n",
              " ('wartawan', 0.43224775791168213),\n",
              " ('kalimantan', 0.42896780371665955)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9YuUmjFGau0"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxjniL5RF_Ba",
        "outputId": "e2897dc3-90c8-40a8-da22-f38d8867108b"
      },
      "source": [
        "%%time\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_train.content)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"Total words\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words 14976\n",
            "CPU times: user 159 ms, sys: 92.5 ms, total: 251 ms\n",
            "Wall time: 147 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sVGPRPYF_7q",
        "outputId": "900edb40-f6a5-4071-c7eb-35f17d6de1f6"
      },
      "source": [
        "%%time\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# KERAS\n",
        "SEQUENCE_LENGTH = 300\n",
        "\n",
        "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.content), maxlen=SEQUENCE_LENGTH)\n",
        "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.content), maxlen=SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 77.7 ms, sys: 0 ns, total: 77.7 ms\n",
            "Wall time: 78.1 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n4vwmzkGBP8",
        "outputId": "4e0ba6d1-1b08-48b5-c61d-08bbbd7a909a"
      },
      "source": [
        "labels = []\n",
        "labels = df_train['Label Text Classification'].unique().tolist()\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yaqw7-mgGCCm",
        "outputId": "7a39be46-15dd-4f6a-c364-6745a0aac019"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(df_train['Label Text Classification'].tolist())\n",
        "\n",
        "y_train = encoder.transform(df_train['Label Text Classification'].tolist())\n",
        "y_test = encoder.transform(df_test['Label Text Classification'].tolist())\n",
        "\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_test = y_test.reshape(-1,1)\n",
        "\n",
        "print(\"y_train\",y_train.shape)\n",
        "print(\"y_test\",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train (495, 1)\n",
            "y_test (124, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riYcHWeMGDw9",
        "outputId": "82c13dd3-e774-44bb-dfd3-56d3ac3888af"
      },
      "source": [
        "print(\"x_train\", x_train.shape)\n",
        "print(\"y_train\", y_train.shape)\n",
        "print()\n",
        "print(\"x_test\", x_test.shape)\n",
        "print(\"y_test\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train (495, 300)\n",
            "y_train (495, 1)\n",
            "\n",
            "x_test (124, 300)\n",
            "y_test (124, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHygXHgKGFIu",
        "outputId": "f7b6b35e-506e-4c40-8e1e-2f716a7c3776"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in w2v_model.wv:\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(14976, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JGyQ1JNGs25",
        "outputId": "bfe4e7e9-01de-471c-af7d-d77f82a3746c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "\n",
        "def make_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 300, 300)          4492800   \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 300, 300)          0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               160400    \n",
            "_________________________________________________________________\n",
            "dense_215 (Dense)            (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 4,653,301\n",
            "Trainable params: 160,501\n",
            "Non-trainable params: 4,492,800\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvbpCntDGuXR"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=\"adam\",\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGGnT0HVGvfR",
        "outputId": "44b52e0d-b6ad-43e4-9ae8-368bd012b4c8"
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: 1e-6 * 10**(epoch / 10), verbose=1)\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[lr_schedule],\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
            "56/56 [==============================] - 35s 565ms/step - loss: 0.7083 - acc: 0.4889 - val_loss: 0.7135 - val_acc: 0.4800\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.2589254117941672e-06.\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 0.7061 - acc: 0.4793 - val_loss: 0.7129 - val_acc: 0.4800\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5848931924611134e-06.\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 0.7060 - acc: 0.4631 - val_loss: 0.7120 - val_acc: 0.4600\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 1.9952623149688796e-06.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.7035 - acc: 0.4822 - val_loss: 0.7110 - val_acc: 0.4800\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.51188643150958e-06.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.7108 - acc: 0.4505 - val_loss: 0.7096 - val_acc: 0.4600\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 3.162277660168379e-06.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.7025 - acc: 0.4985 - val_loss: 0.7080 - val_acc: 0.4600\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 3.981071705534972e-06.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 0.7003 - acc: 0.4836 - val_loss: 0.7060 - val_acc: 0.4600\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 5.011872336272722e-06.\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 0.6970 - acc: 0.5063 - val_loss: 0.7037 - val_acc: 0.4800\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 6.309573444801933e-06.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.6933 - acc: 0.4991 - val_loss: 0.7008 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 7.943282347242815e-06.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 0.7070 - acc: 0.4715 - val_loss: 0.6977 - val_acc: 0.4800\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 9.999999999999999e-06.\n",
            "56/56 [==============================] - 31s 547ms/step - loss: 0.7044 - acc: 0.4899 - val_loss: 0.6932 - val_acc: 0.4600\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 1.2589254117941675e-05.\n",
            "56/56 [==============================] - 31s 546ms/step - loss: 0.6870 - acc: 0.5746 - val_loss: 0.6886 - val_acc: 0.5400\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 1.584893192461113e-05.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.6895 - acc: 0.5221 - val_loss: 0.6828 - val_acc: 0.5800\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 1.9952623149688796e-05.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.6732 - acc: 0.6176 - val_loss: 0.6754 - val_acc: 0.6000\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 2.5118864315095795e-05.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.6659 - acc: 0.5616 - val_loss: 0.6678 - val_acc: 0.5800\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 3.162277660168379e-05.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 0.6615 - acc: 0.5972 - val_loss: 0.6583 - val_acc: 0.6200\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 3.9810717055349735e-05.\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 0.6503 - acc: 0.5894 - val_loss: 0.6482 - val_acc: 0.6400\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 5.0118723362727224e-05.\n",
            "56/56 [==============================] - 31s 554ms/step - loss: 0.6499 - acc: 0.6059 - val_loss: 0.6363 - val_acc: 0.6600\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 6.309573444801932e-05.\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 0.6287 - acc: 0.6394 - val_loss: 0.6230 - val_acc: 0.6400\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 7.943282347242814e-05.\n",
            "56/56 [==============================] - 31s 553ms/step - loss: 0.6120 - acc: 0.6520 - val_loss: 0.6060 - val_acc: 0.6600\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 9.999999999999999e-05.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.6006 - acc: 0.6537 - val_loss: 0.5844 - val_acc: 0.6600\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00012589254117941674.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.5458 - acc: 0.7201 - val_loss: 0.6057 - val_acc: 0.6200\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00015848931924611142.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.5283 - acc: 0.7229 - val_loss: 0.6392 - val_acc: 0.5800\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00019952623149688788.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.4881 - acc: 0.7466 - val_loss: 0.6787 - val_acc: 0.6400\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00025118864315095795.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.4971 - acc: 0.7380 - val_loss: 0.6371 - val_acc: 0.6400\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00031622776601683794.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.5023 - acc: 0.7485 - val_loss: 0.7109 - val_acc: 0.6800\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0003981071705534973.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 0.4476 - acc: 0.7744 - val_loss: 0.7504 - val_acc: 0.6400\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005011872336272724.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.4274 - acc: 0.7922 - val_loss: 0.6365 - val_acc: 0.6400\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.000630957344480193.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.4012 - acc: 0.8091 - val_loss: 0.6900 - val_acc: 0.6200\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0007943282347242812.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.4251 - acc: 0.7818 - val_loss: 0.7433 - val_acc: 0.6200\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.001.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.3435 - acc: 0.8486 - val_loss: 0.6923 - val_acc: 0.6800\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0012589254117941675.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.3896 - acc: 0.8455 - val_loss: 0.8185 - val_acc: 0.6400\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0015848931924611139.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.3529 - acc: 0.8546 - val_loss: 0.7153 - val_acc: 0.7200\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.001995262314968879.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.3112 - acc: 0.8674 - val_loss: 0.7386 - val_acc: 0.6800\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00251188643150958.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.2667 - acc: 0.8891 - val_loss: 0.8516 - val_acc: 0.6400\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.0031622776601683794.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.3492 - acc: 0.8677 - val_loss: 0.6711 - val_acc: 0.7400\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.003981071705534973.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.3275 - acc: 0.8903 - val_loss: 0.5962 - val_acc: 0.7000\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.005011872336272725.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.3262 - acc: 0.8742 - val_loss: 1.0248 - val_acc: 0.6600\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00630957344480193.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: 0.3938 - acc: 0.8326 - val_loss: 0.5316 - val_acc: 0.7600\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.007943282347242814.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: 0.3502 - acc: 0.8547 - val_loss: 0.6443 - val_acc: 0.6800\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.01.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.3322 - acc: 0.8562 - val_loss: 0.6516 - val_acc: 0.7200\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.012589254117941661.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.2815 - acc: 0.8777 - val_loss: 0.9703 - val_acc: 0.6600\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.01584893192461114.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: 0.3316 - acc: 0.8686 - val_loss: 0.9732 - val_acc: 0.6200\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.01995262314968879.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: 0.3212 - acc: 0.8756 - val_loss: 0.7334 - val_acc: 0.6600\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.025118864315095822.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: 0.4126 - acc: 0.8221 - val_loss: 0.7459 - val_acc: 0.5600\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.03162277660168379.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.5702 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.039810717055349686.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4550 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.05011872336272725.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4810 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.0630957344480193.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: nan - acc: 0.4426 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.07943282347242821.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4282 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.09999999999999999.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4529 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.12589254117941662.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4952 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1584893192461114.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4326 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1995262314968879.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4557 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.2511886431509582.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4921 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.3162277660168379.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4797 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.3981071705534969.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4830 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.5011872336272725.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4775 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.6309573444801929.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.5167 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.7943282347242822.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.5040 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 1.0.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.5039 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 1.258925411794166.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4540 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 1.584893192461114.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4520 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 1.9952623149688788.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4649 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 2.5118864315095824.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.5020 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 3.1622776601683795.\n",
            "56/56 [==============================] - 31s 547ms/step - loss: nan - acc: 0.4519 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 3.981071705534969.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4681 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 5.011872336272725.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4980 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 6.30957344480193.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4979 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 7.943282347242822.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4638 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 10.0.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4635 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 12.589254117941662.\n",
            "56/56 [==============================] - 31s 553ms/step - loss: nan - acc: 0.4985 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 15.848931924611142.\n",
            "56/56 [==============================] - 31s 554ms/step - loss: nan - acc: 0.4655 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 19.952623149688787.\n",
            "56/56 [==============================] - 31s 556ms/step - loss: nan - acc: 0.4913 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 25.118864315095824.\n",
            "56/56 [==============================] - 31s 553ms/step - loss: nan - acc: 0.4336 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 31.62277660168379.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4809 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 39.81071705534969.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4387 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 50.118723362727245.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4559 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 63.09573444801929.\n",
            "56/56 [==============================] - 31s 555ms/step - loss: nan - acc: 0.4317 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 79.43282347242821.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4662 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 100.0.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4635 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 125.89254117941661.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4590 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 158.48931924611108.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.5118 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 199.52623149688827.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4675 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 251.1886431509582.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4438 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 316.22776601683796.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4500 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 398.10717055349687.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4912 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 501.18723362727144.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4696 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 630.9573444801943.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4554 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 794.3282347242821.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4677 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 1000.0.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4444 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 1258.925411794166.\n",
            "56/56 [==============================] - 31s 552ms/step - loss: nan - acc: 0.4693 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 1584.8931924611109.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4740 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 1995.2623149688827.\n",
            "56/56 [==============================] - 31s 546ms/step - loss: nan - acc: 0.4842 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 2511.886431509582.\n",
            "56/56 [==============================] - 31s 548ms/step - loss: nan - acc: 0.4721 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 3162.277660168379.\n",
            "56/56 [==============================] - 31s 550ms/step - loss: nan - acc: 0.4707 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 3981.071705534969.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4449 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 5011.872336272714.\n",
            "56/56 [==============================] - 31s 549ms/step - loss: nan - acc: 0.4538 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 6309.573444801943.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4630 - val_loss: nan - val_acc: 0.5000\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 7943.282347242822.\n",
            "56/56 [==============================] - 31s 551ms/step - loss: nan - acc: 0.4003 - val_loss: nan - val_acc: 0.5000\n",
            "CPU times: user 1h 31min 22s, sys: 6min 19s, total: 1h 37min 41s\n",
            "Wall time: 51min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "nns67adWHDFT",
        "outputId": "d5168e72-fad9-48c8-f4d8-ecaa8d5164d1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel('learning rate')\n",
        "plt.ylabel('val_loss')\n",
        "plt.semilogx(history.history[\"lr\"], history.history[\"val_loss\"])\n",
        "plt.axis([1e-6, 1e0, 0, 2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1e-06, 1.0, 0.0, 2.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--DQY5QYHFzT",
        "outputId": "f4ca5a97-9d54-4a3e-edaa-6d8357ad5d86"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
        "\n",
        "model = make_model()\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0031), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train,validation_data=(x_test,y_test), batch_size=8, epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "62/62 [==============================] - 39s 578ms/step - loss: 0.6766 - accuracy: 0.5651 - val_loss: 0.6185 - val_accuracy: 0.6855\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 35s 566ms/step - loss: 0.5495 - accuracy: 0.7148 - val_loss: 0.6425 - val_accuracy: 0.6532\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 35s 568ms/step - loss: 0.4736 - accuracy: 0.7792 - val_loss: 0.6993 - val_accuracy: 0.6290\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 35s 566ms/step - loss: 0.3991 - accuracy: 0.8284 - val_loss: 0.6528 - val_accuracy: 0.6532\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 35s 565ms/step - loss: 0.3656 - accuracy: 0.8381 - val_loss: 0.7313 - val_accuracy: 0.6452\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 35s 569ms/step - loss: 0.3114 - accuracy: 0.8707 - val_loss: 0.7710 - val_accuracy: 0.6210\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 35s 568ms/step - loss: 0.2180 - accuracy: 0.9198 - val_loss: 0.8137 - val_accuracy: 0.6532\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 35s 567ms/step - loss: 0.2273 - accuracy: 0.9029 - val_loss: 0.7769 - val_accuracy: 0.6613\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 35s 566ms/step - loss: 0.1619 - accuracy: 0.9533 - val_loss: 0.8640 - val_accuracy: 0.6613\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 35s 570ms/step - loss: 0.1799 - accuracy: 0.9346 - val_loss: 0.8571 - val_accuracy: 0.6935\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 35s 565ms/step - loss: 0.1017 - accuracy: 0.9696 - val_loss: 0.9264 - val_accuracy: 0.6774\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 35s 569ms/step - loss: 0.1041 - accuracy: 0.9731 - val_loss: 0.9022 - val_accuracy: 0.6694\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 35s 566ms/step - loss: 0.0930 - accuracy: 0.9848 - val_loss: 1.0784 - val_accuracy: 0.6532\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 35s 567ms/step - loss: 0.0673 - accuracy: 0.9850 - val_loss: 1.2230 - val_accuracy: 0.6532\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 35s 569ms/step - loss: 0.0616 - accuracy: 0.9849 - val_loss: 1.2938 - val_accuracy: 0.6532\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 35s 570ms/step - loss: 0.0448 - accuracy: 0.9900 - val_loss: 1.1583 - val_accuracy: 0.6532\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 35s 568ms/step - loss: 0.0527 - accuracy: 0.9810 - val_loss: 1.1366 - val_accuracy: 0.6694\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 35s 570ms/step - loss: 0.0418 - accuracy: 0.9891 - val_loss: 1.3837 - val_accuracy: 0.6855\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 35s 569ms/step - loss: 0.0427 - accuracy: 0.9845 - val_loss: 1.3211 - val_accuracy: 0.6452\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 35s 571ms/step - loss: 0.0395 - accuracy: 0.9868 - val_loss: 1.4141 - val_accuracy: 0.6774\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696FtXIzHHWH",
        "outputId": "d100eab7-2afb-44e8-d062-ce2ad0bcd360"
      },
      "source": [
        "%%time\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
        "print()\n",
        "print(\"ACCURACY:\",score[1])\n",
        "print(\"LOSS:\",score[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 1s 66ms/step - loss: 1.4141 - accuracy: 0.6774\n",
            "\n",
            "ACCURACY: 0.6774193644523621\n",
            "LOSS: 1.4141312837600708\n",
            "CPU times: user 1.9 s, sys: 20.5 ms, total: 1.92 s\n",
            "Wall time: 1.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}